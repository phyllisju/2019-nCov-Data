state2 = list(chain_state = chain_state2, current_pdf = current_pdf2),
identical = identical_))
}
## coupling of L-step MH, using max coupling of proposal
## and naive coupling of the 'L' variable (same L for both chains)
coupled_Lstep_MH_kernel <- function(state1, state2){
L <- sample(1:Lmax, 1)
for (ell in 1:L){
coupled_results <- coupled_MH_kernel(state1, state2)
state1 <- coupled_results$state1
state2 <- coupled_results$state2
identical_ <- coupled_results$identical
}
return(list(state1 = state1, state2 = state2, identical = identical_))
}
## more sophisticated coupling
## where OT is used to jointly sample L
## so that the selected next states would be as close as possible on average
ot_coupled_Lstep_MH_kernel <- function(state1, state2){
chain_states1 <- matrix(nrow = Lmax, ncol = target_dim)
chain_states2 <- matrix(nrow = Lmax, ncol = target_dim)
current_pdf1 <- rep(0, Lmax)
current_pdf2 <- rep(0, Lmax)
for (ell in 1:Lmax){
coupled_results <- coupled_MH_kernel(state1, state2)
state1 <- coupled_results$state1
state2 <- coupled_results$state2
chain_states1[ell,] <- state1$chain_state
chain_states2[ell,] <- state2$chain_state
current_pdf1[ell] <- state1$current_pdf
current_pdf2[ell] <- state2$current_pdf
}
## Now sample L1 and L2 so that
## marginally L1 and L2 ~ {1,...,Lmax}
## but jointly minimizes expected distance between chain_states1[L1] and chain_states2[L2]
pairwisedistances <- rdist::cdist(chain_states1, chain_states2)
capture.output({transport_result <- transport::transport(a = rep(1/Lmax, Lmax), b = rep(1/Lmax, Lmax), costm = pairwisedistances,
method = "shortsimplex")})
transport_plan <- matrix(0,Lmax,Lmax)
transport_plan[cbind(transport_result$from,transport_result$to)] <-  transport_result$mass
L1L2 <- arrayInd(sample.int(Lmax*Lmax, size = 1, replace = TRUE, prob = as.vector(transport_plan)), dim(transport_plan))
L1 <- L1L2[1]
L2 <- L1L2[2]
state1 <- list(chain_state = chain_states1[L1,], current_pdf = current_pdf1[L1])
state2 <- list(chain_state = chain_states2[L2,], current_pdf = current_pdf2[L2])
identical_ <- identical(state1$chain_state, state2$chain_state)
return(list(state1 = state1, state2 = state2, identical = identical_))
}
MTM_kernel <- function(state){
## use Barker's choince
chain_state <- state$chain_state
current_pdf <- state$current_pdf
# L <- Lmax
L <- sample.int(n = Lmax, size = 1)
proposals <- matrix(data = NA, nrow = L, ncol = target_dim)
proposalpdfs <- rep(NA, L)
for (ell in 1:L){
proposals[ell,] <- fast_rmvnorm_chol(1, chain_state, Sigma_proposal_chol)
}
proposalpdfs <- apply(proposals, 1, function(x) logtarget(x))
proposalweights <- proposalpdfs # may choose other weight functions
proposalweights_max <- max(proposalweights)
proposalweights <- proposalweights - proposalweights_max
ws <- exp(proposalweights) / (sum(exp(proposalweights))) # normalized weights
y <- proposals[sample.int(n = L, size = 1, replace = F, prob = ws),]
auxilliary_proposals <- matrix(data = NA, nrow = L, ncol = target_dim)
auxilliary_proposals[L,] <- chain_state
for (ell in 1:(L-1)){
auxilliary_proposals[ell,] <- fast_rmvnorm_chol(1, chain_state, Sigma_proposal_chol)
}
auxilliarypdfs <- apply(auxilliary_proposals, 1, logtarget)
auxilliaryweights <- auxilliarypdfs
auxilliaryweights_max <- max(auxilliaryweights)
auxilliaryweights <- auxilliaryweights - auxilliaryweights_max
logalpha <- log(sum(exp(proposalweights))) - log(sum(exp(auxilliaryweights))) + proposalweights_max - auxilliaryweights_max
if (log(runif(1)) < logalpha){
return(list(chain_state = y, current_pdf = logtarget(y)))
} else {
return(list(chain_state = chain_state, current_pdf = current_pdf))
}
}
## how to couple two MTM kernels?
coupled_Lstep_MTM_kernel <- function(state1, state2){
L <- sample(1:Lmax, 1)
for (ell in 1:L){
coupled_results <- coupled_MH_kernel(state1, state2)
state1 <- coupled_results$state1
state2 <- coupled_results$state2
identical_ <- coupled_results$identical
}
return(list(state1 = state1, state2 = state2, identical = identical_))
}
## use ot to couple, need the Barker's choice formula
ot_coupled_MTM_kernel <- function(state1, state2){
proposal_states1 <- matrix(nrow = Lmax, ncol = target_dim)
proposal_states2 <- matrix(nrow = Lmax, ncol = target_dim)
proposal_pdf1 <- rep(0, Lmax)
proposal_pdf2 <- rep(0, Lmax)
chain_state1 <- state1$chain_state
chain_state2 <- state2$chain_state
auxilliary_w1 <- matrix(state1$current_pdf, nrow = Lmax - 1, ncol = Lmax -1)
auxilliary_w2 <- matrix(state2$current_pdf, nrow = Lmax - 1, ncol = Lmax -1)
for (ell in 1:(Lmax-1)){
proposal_states1[ell,] <- fast_rmvnorm_chol(1, chain_state1, Sigma_proposal_chol)
proposal_states2[ell,] <- fast_rmvnorm_chol(1, chain_state1, Sigma_proposal_chol)
proposal_pdf1[ell] <- logtarget(proposal_states1[ell,])
proposal_pdf2[ell] <- logtarget(proposal_states1[ell,])
for (ellaux in 1:(Lmax-2)){
auxilliary_w1[ell,ellaux] <- logtarget(fast_rmvnorm(1, proposal_states1[ell,]), Sigma_proposal_chol)
auxilliary_w2[ell,ellaux] <- logtarget(fast_rmvnorm(1, proposal_states2[ell,]), Sigma_proposal_chol)
}
}
proposal_states1[Lmax,] <- chain_state1
proposal_states2[Lmax,] <- chain_state2
proposal_pdf1[Lmax] <- state1$current_pdf
proposal_pdf2[Lmax] <- state2$current_pdf
## calculate the weights
w1 <- rep(0, Lmax)
w2 <- rep(0, Lmax)
proposal1_max <- max(proposal_pdf1, auxilliary_w1)
proposal2_max <- max(proposal_pdf2, auxilliary_w2)
proposal_pdf1 <- proposal_pdf1 - proposal1_max
proposal_pdf2 <- proposal_pdf2 - proposal2_max
auxilliary_w1 <- auxilliary_w1 - proposal1_max
auxilliary_w2 <- auxilliary_w2 - proposal2_max
for (ell in 1:(Lmax-1)){
w1[ell] <- exp(proposal1_pdf[ell])/(sum(exp(proposal_pdf1)) + sum(exp(auxilliary_w1[ell,])))
w2[ell] <- exp(proposal2_pdf[ell])/(sum(exp(proposal_pdf1)) + sum(exp(auxilliary_w2[ell,])))
}
w1[Lmax] <- 1 - sum(w1)
w2[Lmax] <- 1 - sum(w2)
## Now sample L1 and L2 so that
## marginally L1 and L2 ~ {1,...,Lmax}
## but jointly minimizes expected distance between chain_states1[L1] and chain_states2[L2]
pairwisedistances <- rdist::cdist(proposal_states1, proposal_states2)
capture.output({transport_result <- transport::transport(a = w1, b = w2, costm = pairwisedistances,
method = "shortsimplex")})
transport_plan <- matrix(0,Lmax,Lmax)
transport_plan[cbind(transport_result$from,transport_result$to)] <-  transport_result$mass
L1L2 <- arrayInd(sample.int(Lmax*Lmax, size = 1, replace = TRUE, prob = as.vector(transport_plan)), dim(transport_plan))
L1 <- L1L2[1]
L2 <- L1L2[2]
state1 <- list(chain_state = proposal_states[L1,], current_pdf = proposal_pdf1[L1])
state2 <- list(chain_state = chain_states2[L2,], current_pdf = proposal_pdf2[L2])
identical_ <- identical(state1$chain_state, state2$chain_state)
return(list(state1 = state1, state2 = state2, identical = identical_))
}
return(list(rinit = rinit, MH_kernel= MH_kernel, MTM_kernel = MTM_kernel, Lstep_MH_kernel = Lstep_MH_kernel,
coupled_Lstep_MH_kernel = coupled_Lstep_MH_kernel, ot_coupled_Lstep_MH_kernel = ot_coupled_Lstep_MH_kernel,
ot_coupled_MTM_kernel = ot_coupled_MTM_kernel))
}
nrep <- 5e2
# dimensions <- c(2,4,6,8)
Lmax <- 30
results <- data.frame()
pb <- get_problem(target_dim = 2, Lmax = 10, "max")
ot_meetingtimes <- foreach(irep = 1:nrep, .combine = c) %dorng% {
sample_coupled_chains(pb$MTM_kernel, pb$ot_coupled_MTM, pb$rinit, max_iterations = 1e5)$meetingtime
}
## Instead of doing one step of MH, we can do L tries
## with L drawn randomly between 1 and L_max or set to a fixed value
## This might lead to an algorithm for which coupling can be made with OT
## MH on a Normal target
library(unbiasedmcmc)
library(doParallel)
library(doRNG)
library(transport)
# register parallel cores
cl <- makeCluseter(cores = detectCores() - 2)
registerDoParallel(cl)
# stopCluster(cl) # stop parallel
# registerDoParallel(cores = 2)
# registerDoParallel(cores = detectCores()-2)
# set RNG seed
set.seed(1)
Lmax <- 10
target_dim <- 3
mean_pi <- rep(0, target_dim)
Sigma_pi <- diag(1, target_dim, target_dim)
Sigma_pi_chol <- chol(Sigma_pi)
Sigma_pi_chol_inv <- solve(Sigma_pi_chol)
logtarget <- function(x) fast_dmvnorm_chol_inverse(matrix(x, nrow = 1), mean_pi, Sigma_pi_chol_inv)
# initial distribution
rinit <- function(){
chain_state <- rnorm(target_dim, 1, 1)
current_pdf <- logtarget(chain_state)
return(list(chain_state = chain_state, current_pdf = current_pdf))
}
Sigma_proposal <- Sigma_pi/target_dim
Sigma_proposal_chol <- chol(Sigma_proposal)
Sigma_proposal_chol_inv <- solve(Sigma_proposal_chol)
MH_kernel <- function(state){
chain_state <- state$chain_state
current_pdf <- state$current_pdf
proposal <- fast_rmvnorm_chol(1, chain_state, Sigma_proposal_chol)
proposal_pdf <- logtarget(proposal)
if (log(runif(1)) < (proposal_pdf - current_pdf)){
return(list(chain_state = proposal, current_pdf = proposal_pdf))
} else {
return(list(chain_state = chain_state, current_pdf = current_pdf))
}
}
# MTM kernel
MTM_kernel <- function(state){
## use Barker's choince
chain_state <- state$chain_state
current_pdf <- state$current_pdf
# L <- Lmax
L <- sample.int(n = Lmax, size = 1)
proposals <- matrix(data = NA, nrow = L, ncol = target_dim)
proposalpdfs <- rep(NA, L)
for (ell in 1:L){
proposals[ell,] <- fast_rmvnorm_chol(1, chain_state, Sigma_proposal_chol)
}
proposalpdfs <- apply(proposals, 1, function(x) logtarget(x))
proposalweights <- proposalpdfs # may choose other weight functions
proposalweights_max <- max(proposalweights)
proposalweights <- proposalweights - proposalweights_max
ws <- exp(proposalweights) / (sum(exp(proposalweights))) # normalized weights
y <- proposals[sample.int(n = L, size = 1, replace = F, prob = ws),]
auxilliary_proposals <- matrix(data = NA, nrow = L, ncol = target_dim)
auxilliary_proposals[L,] <- chain_state
for (ell in 1:(L-1)){
auxilliary_proposals[ell,] <- fast_rmvnorm_chol(1, chain_state, Sigma_proposal_chol)
}
auxilliarypdfs <- apply(auxilliary_proposals, 1, logtarget)
auxilliaryweights <- auxilliarypdfs
auxilliaryweights_max <- max(auxilliaryweights)
auxilliaryweights <- auxilliaryweights - auxilliaryweights_max
logalpha <- log(sum(exp(proposalweights))) - log(sum(exp(auxilliaryweights))) + proposalweights_max - auxilliaryweights_max
if (log(runif(1)) < logalpha){
return(list(chain_state = y, current_pdf = logtarget(y)))
} else {
return(list(chain_state = chain_state, current_pdf = current_pdf))
}
}
# Lmax <- 2
Lstep_MH_kernel <- function(state){
L <- sample(1:Lmax, 1)
for (ell in 1:L) state <- MH_kernel(state)
return(state)
}
niterations <- 10000
mhchain <- rep(0, niterations)
state <- rinit()
for (i in 1:niterations){
state <- MTM_kernel(state)
mhchain[i] <- state$chain_state[1]
}
## histogram of the chain after removing 100 first iterations
hist(mhchain[1001:niterations], prob = TRUE, nclass = 140, main = "", xlab = "x")
curve(dnorm(x), add = TRUE, col = "red")
stopCluster(cl)
source('~/Dropbox/OTcouplingmultipleproposals/OTcoupledmtm.R', echo=TRUE)
lilibrary(devtools  )
library(devtools)
library(roxygen2)
document()
document()
library(roxygen2)
devtools::install_github("qingyuanzhao/2019-nCov-Data")
library(nCoV2019.data)
data(cases.in.china)
data(cases.outside.china)
devtools::install_github("qingyuanzhao/2019-nCov-Data")
source("../R/functions.R")
setwd("~/Dropbox/wuhan/2019-nCov-Data/scripts")
source("../R/functions.R")
data <- read.table("Feb10InChina.tsv", sep = "\t", header = TRUE)
data <- read.table("../1st-Report/Feb10InChina.tsv", sep = "\t", header = TRUE)
data$Confirmed <- date.process(data$Confirmed)
data$Arrived <- date.process(data$Arrived)
data$Symptom <- date.process(data$Symptom)
data$Initial <- date.process(data$Initial)
data$Hospital <- date.process(data$Hospital)
case72 <- data[72,]
data <- data[-c(72,19,95), ] # Don't know how to parse these infected date yet
data <- parse.infected(data)
case72$Infected_first <- date.process("15-Jan")
case72$Infected_last <- date.process("21-Jan")
data <- rbind(data, case72)
## Only consider cases with known symptom onset
data <- subset(data, !is.na(Symptom))
## only consider cases arrived on or before January 23
# data <- subset(data, Arrived <= 23+31) ## this will remove outside cases
## remove cases with no infection informations
# data <- subset(data, !(is.na(Arrived) & Infected_first == 1 & Infected_last == Symptom)) # remove cases with no information
# data <- subset(data, !(is.na(Arrived))) ## only consider outside cases
data <- subset(data, Infected_first != 1)
data <- subset(data, Infected_last != Symptom)
dim(data)
hist(data$Symptom - data$Infected_last)
hist(data$Symptom - data$Infected_first)
hist(data$Infected_last - data$Infected_first)
#' Compute the likelihood
#'
#' GT is a discretized distribution for the generation time
#'
infection.likelihood <- function(symptom, infected_first, infected_last, GT) {
loglike <- 0
for (i in 1:nrow(data)) {
min.incub <- symptom[i] - infected_last[i]
max.incub <- symptom[i] - infected_first[i]
loglike <- loglike + log(sum(GT$GT[1 +(min.incub):(max.incub)]))
}
loglike
}
myfun <- function(par) {
GT <- R0::generation.time("lognormal", par,truncate = 100);
infection.likelihood(data$Symptom, data$Infected_first, data$Infected_last, GT)
}
fit <- optim(c(7.5, 3.4), myfun, control = list(fnscale = -1))
print(fit)
pars <- expand.grid(mean = seq(3, 15, 0.05), sd = seq(2, 8, 0.05))
pars$loglike <- apply(pars, 1, myfun)
print(pars[which.max(pars$loglike),]) ## print the grid-search MLE to terminal
pars$in.CR <- (pars$loglike > fit$value - qchisq(0.95, 1) / 2)
library(ggplot2)
p1 <- ggplot(pars) + aes(x = mean, y = sd, fill = loglike) + geom_tile()
p1
p2 <- ggplot(pars) + aes(x = mean, y = sd, fill = in.CR) +geom_tile()
p2
hist(data$Infected_last - data$Infected_first)
summary(data$Infected_last - data$Infected_first)
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source("../R/functions.R")
data <- read.table("../1st-Report/Feb10InChina.tsv", sep = "\t", header = TRUE)
data$Confirmed <- date.process(data$Confirmed)
data$Arrived <- date.process(data$Arrived)
data$Symptom <- date.process(data$Symptom)
data$Initial <- date.process(data$Initial)
data$Hospital <- date.process(data$Hospital)
case72 <- data[72,]
data <- data[-c(72,19,95), ] # Don't know how to parse these infected date yet
data <- parse.infected(data)
case72$Infected_first <- date.process("15-Jan")
case72$Infected_last <- date.process("21-Jan")
data <- rbind(data, case72)
## Only consider cases with known symptom onset
data <- subset(data, !is.na(Symptom))
## only consider cases arrived on or before January 23
# data <- subset(data, Arrived <= 23+31) ## this will remove outside cases
## remove cases with no infection informations
# data <- subset(data, !(is.na(Arrived) & Infected_first == 1 & Infected_last == Symptom)) # remove cases with no information
# data <- subset(data, !(is.na(Arrived))) ## only consider outside cases
data <- subset(data, Infected_first != 1)
data <- subset(data, Infected_last != Symptom)
dim(data)
hist(data$Symptom - data$Infected_last)
hist(data$Symptom - data$Infected_first)
hist(data$Infected_last - data$Infected_first)
summary(data$Infected_last - data$Infected_first)
#' Compute the likelihood
#'
#' GT is a discretized distribution for the generation time
#'
infection.likelihood <- function(symptom, infected_first, infected_last, GT) {
loglike <- 0
for (i in 1:nrow(data)) {
min.incub <- symptom[i] - infected_last[i]
max.incub <- symptom[i] - infected_first[i]
loglike <- loglike + log(sum(GT$GT[1 +(min.incub):(max.incub)]))
}
loglike
}
myfun <- function(par) {
GT <- R0::generation.time("gamma", par,truncate = 100);
infection.likelihood(data$Symptom, data$Infected_first, data$Infected_last, GT)
}
fit <- optim(c(7.5, 3.4), myfun, control = list(fnscale = -1))
print(fit)
pars <- expand.grid(mean = seq(3, 15, 0.05), sd = seq(2, 8, 0.05))
pars$loglike <- apply(pars, 1, myfun)
print(pars[which.max(pars$loglike),]) ## print the grid-search MLE to terminal
pars$in.CR <- (pars$loglike > fit$value - qchisq(0.95, 1) / 2)
library(ggplot2)
p1 <- ggplot(pars) + aes(x = mean, y = sd, fill = loglike) + geom_tile()
p1
p2 <- ggplot(pars) + aes(x = mean, y = sd, fill = in.CR) +geom_tile()
p2
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
p1
p2
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source("../R/functions.R")
data <- read.table("../1st-Report/Feb10InChina.tsv", sep = "\t", header = TRUE)
data$Confirmed <- date.process(data$Confirmed)
data$Arrived <- date.process(data$Arrived)
data$Symptom <- date.process(data$Symptom)
data$Initial <- date.process(data$Initial)
data$Hospital <- date.process(data$Hospital)
case72 <- data[72,]
data <- data[-c(72,19,95), ] # Don't know how to parse these infected date yet
data <- parse.infected(data)
case72$Infected_first <- date.process("15-Jan")
case72$Infected_last <- date.process("21-Jan")
data <- rbind(data, case72)
## Only consider cases with known symptom onset
data <- subset(data, !is.na(Symptom))
## only consider cases arrived on or before January 23
# data <- subset(data, Arrived <= 23+31) ## this will remove outside cases
## remove cases with no infection informations
# data <- subset(data, !(is.na(Arrived) & Infected_first == 1 & Infected_last == Symptom)) # remove cases with no information
# data <- subset(data, !(is.na(Arrived))) ## only consider outside cases
data <- subset(data, Infected_first != 1)
data <- subset(data, Infected_last != Symptom)
dim(data)
hist(data$Symptom - data$Infected_last)
hist(data$Symptom - data$Infected_first)
hist(data$Infected_last - data$Infected_first)
summary(data$Infected_last - data$Infected_first)
#' Compute the likelihood
#'
#' GT is a discretized distribution for the generation time
#'
infection.likelihood <- function(symptom, infected_first, infected_last, GT) {
loglike <- 0
for (i in 1:nrow(data)) {
min.incub <- symptom[i] - infected_last[i]
max.incub <- symptom[i] - infected_first[i]
loglike <- loglike + log(sum(GT$GT[1 +(min.incub):(max.incub)]))
}
loglike
}
myfun <- function(par) {
GT <- R0::generation.time("gamma", par,truncate = 100);
infection.likelihood(data$Symptom, data$Infected_first, data$Infected_last, GT)
}
fit <- optim(c(7.5, 3.4), myfun, control = list(fnscale = -1))
print(fit)
pars <- expand.grid(mean = seq(3, 15, 0.05), sd = seq(2, 8, 0.05))
pars$loglike <- apply(pars, 1, myfun)
print(pars[which.max(pars$loglike),]) ## print the grid-search MLE to terminal
pars$in.CR <- (pars$loglike > fit$value - qchisq(0.95, 1) / 2)
library(ggplot2)
p1 <- ggplot(pars) + aes(x = mean, y = sd, fill = loglike) + geom_tile()
# p1
p2 <- ggplot(pars) + aes(x = mean, y = sd, fill = in.CR) +geom_tile()
p2
?R0::generation.time
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
summary(data$Symptom[i] - data$Infected_first)
summary(data$Symptom - data$Infected_last)
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source("../R/functions.R")
data <- read.table("../1st-Report/Feb10InChina.tsv", sep = "\t", header = TRUE)
data$Confirmed <- date.process(data$Confirmed)
data$Arrived <- date.process(data$Arrived)
data$Symptom <- date.process(data$Symptom)
data$Initial <- date.process(data$Initial)
data$Hospital <- date.process(data$Hospital)
case72 <- data[72,]
data <- data[-c(72,19,95), ] # Don't know how to parse these infected date yet
data <- parse.infected(data)
case72$Infected_first <- date.process("15-Jan")
case72$Infected_last <- date.process("21-Jan")
data <- rbind(data, case72)
## Only consider cases with known symptom onset
data <- subset(data, !is.na(Symptom))
## only consider cases arrived on or before January 23
# data <- subset(data, Arrived <= 23+31) ## this will remove outside cases
## remove cases with no infection informations
# data <- subset(data, !(is.na(Arrived) & Infected_first == 1 & Infected_last == Symptom)) # remove cases with no information
# data <- subset(data, !(is.na(Arrived))) ## only consider outside cases
data <- subset(data, Infected_first != 1)
data <- subset(data, Infected_last != Symptom)
dim(data)
hist(data$Symptom - data$Infected_last)
hist(data$Symptom - data$Infected_first)
hist(data$Infected_last - data$Infected_first)
summary(data$Infected_last - data$Infected_first)
#' Compute the likelihood
#'
#' GT is a discretized distribution for the generation time
#'
infection.likelihood <- function(symptom, infected_first, infected_last, GT) {
loglike <- 0
for (i in 1:nrow(data)) {
min.incub <- symptom[i] - infected_last[i]
max.incub <- symptom[i] - infected_first[i]
loglike <- loglike + log(sum(GT$GT[(min.incub):(max.incub)])) ## should + 1 be included
}
loglike
}
myfun <- function(par) {
GT <- R0::generation.time("gamma", par,truncate = 100);
infection.likelihood(data$Symptom, data$Infected_first, data$Infected_last, GT)
}
fit <- optim(c(7.5, 3.4), myfun, control = list(fnscale = -1))
print(fit)
pars <- expand.grid(mean = seq(3, 15, 0.05), sd = seq(2, 8, 0.05))
pars$loglike <- apply(pars, 1, myfun)
print(pars[which.max(pars$loglike),]) ## print the grid-search MLE to terminal
pars$in.CR <- (pars$loglike > fit$value - qchisq(0.95, 1) / 2)
library(ggplot2)
p1 <- ggplot(pars) + aes(x = mean, y = sd, fill = loglike) + geom_tile()
# p1
p2 <- ggplot(pars) + aes(x = mean, y = sd, fill = in.CR) +geom_tile()
p2
?R0::generation.time
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
p2
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
source('~/Dropbox/wuhan/2019-nCov-Data/scripts/incubation_estimate.R')
